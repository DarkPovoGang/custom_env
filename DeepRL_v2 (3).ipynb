{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ot333' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mot333\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ot333' is not defined"
     ]
    }
   ],
   "source": [
    "porth ot333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DarkPovoGang/DeepRL/blob/main/DeepRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0lPQfLxp6EU"
   },
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpMI7nrpppn2"
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "model, preprocess = clip.load(\"RN50\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQkhbZucR7eZ",
    "outputId": "da9313ce-1c8c-41e1-c69b-8c2c9b4fc254"
   },
   "outputs": [],
   "source": [
    "# dataset = RefCOCOg('.', 'val')\n",
    "# for i in range(1):\n",
    "#   x = dataset[i]\n",
    "#   print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKroxpH0qnrG"
   },
   "source": [
    "## Our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_VKW0jTYc4LR",
    "outputId": "0f410a60-8f3c-422f-843c-cedf8af28caf"
   },
   "outputs": [],
   "source": [
    "# !apt-get install -y xvfb python-opengl swig x11-utils\n",
    "# !pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k_kBbwDrR8cQ"
   },
   "outputs": [],
   "source": [
    "# from enum import Enum\n",
    "# # class syntax\n",
    "# class Actions(Enum):\n",
    "#   ACT_RT = 0 #Right\n",
    "#   ACT_LT = 1 #Left\n",
    "#   ACT_UP = 2 #Up\n",
    "#   ACT_DN = 3 #Down\n",
    "#   ACT_TA = 4 #Taller\n",
    "#   ACT_FA = 5 #Fatter\n",
    "#   ACT_SR = 6 #Shorter\n",
    "#   ACT_TH = 7 #Thiner\n",
    "#   ACT_TR = 8 #Trigger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registring Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Xfi4D9z39mA7"
   },
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "id": "Xfi4D9z39mA7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Added <TimeLimit<VisualGroundingEnv<DeepLearningProject/VisualGrounding-v0>>> inside registry\n"
     ]
    }
   ],
   "source": [
    "env_dict = gym.envs.registration.registry.env_specs.copy()\n",
    "for env in env_dict:\n",
    "    if 'DeepLearningProject/VisualGrounding-v0' in env:\n",
    "        print(\"Remove {} from registry\".format(env))\n",
    "        del gym.envs.registration.registry.env_specs[env]\n",
    "register(\n",
    "    id='DeepLearningProject/VisualGrounding-v0', \n",
    "    entry_point='custom_env:VisualGroundingEnv',\n",
    "    max_episode_steps=300\n",
    ")\n",
    "\n",
    "env = gym.make('DeepLearningProject/VisualGrounding-v0', split='val')\n",
    "print(\"Added {} inside registry\".format(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKRUOZc6baAh"
   },
   "source": [
    "### Network, Agent and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "31mKYCXhZyZf"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CxCoa4TUZ0L3"
   },
   "outputs": [],
   "source": [
    "#TODO: change\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_features, n_actions, features=24):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # multi layer perceptron\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_features, features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features, features * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features * 2, features * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features * 4, features * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features * 2, features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Sd7GcGt6Z4kO"
   },
   "outputs": [],
   "source": [
    "# create a subclass of Tuple with named attributes representing experience\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "\n",
    "        # represent the buffer as a deque\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "\n",
    "        # add the current experience to the buffer\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        # sample an index for each element in the batch\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "\n",
    "        # extract experience entries for each element in the batch\n",
    "        # each value returned by zip is a list of length batch_size\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        \n",
    "        # print(\"REW:\",rewards)\n",
    "        \n",
    "        # return results as numpy arrays\n",
    "        return np.array(states), \\\n",
    "               np.array(actions), \\\n",
    "               np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), \\\n",
    "               np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJQe0pCDZ8gJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MomYFhu3Z6fX"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        # restarts the environment and reset the accumulated reward\n",
    "        self.state, self.info = self.env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "\n",
    "        # no need to create a computational graph when gathering experience\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # will contain the total reward for the episode if the episode ends\n",
    "            # or None otherwise\n",
    "            done_reward = None\n",
    "\n",
    "            # sample the action randomly with probability epsilon\n",
    "            if np.random.random() < epsilon:\n",
    "                action = self.env.action_space.sample()\n",
    "\n",
    "            # otherwise, select action based on qvalues\n",
    "            else:\n",
    "\n",
    "                # creates a batch made of a single state\n",
    "                # state_a = np.array([self.state], copy=False)\n",
    "                state_tensor = torch.tensor(self.state['agent']).unsqueeze(0).to(device)\n",
    "\n",
    "                # get qvalues and select the index of the maximum\n",
    "                q_values = net(state_tensor)\n",
    "                _, action_tensor = torch.max(q_values, dim=2)\n",
    "                action = int(action_tensor.item())\n",
    "                \n",
    "\n",
    "\n",
    "            # perform a step in the environment\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            # convert to toarch float\n",
    "            new_state['agent'] = torch.tensor(new_state['agent']).clone().detach().float()\n",
    "            self.total_reward += reward\n",
    "\n",
    "            # save the new experience\n",
    "            exp = Experience(self.state, action, reward, is_done, new_state['agent'])\n",
    "            self.exp_buffer.append(exp)\n",
    "\n",
    "            # registers the current state\n",
    "            self.state['agent'] = new_state['agent']\n",
    "\n",
    "            # Gets the current representation of the environment\n",
    "            # current_rgb_image = self.env.render(mode='rgb_array')\n",
    "\n",
    "            # if the episode is finished, reset the environment\n",
    "            if is_done:\n",
    "                done_reward = self.total_reward\n",
    "                self._reset()\n",
    "\n",
    "            return done_reward #, current_rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMWB3mAZcIaz"
   },
   "source": [
    "### Train Loop and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cLizKhHVZ-Sj"
   },
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"DeepLearningProject/VisualGrounding-v0\"\n",
    "\n",
    "# we terminate training if the model on average balances the pole\n",
    "# for at least 195 steps\n",
    "MEAN_REWARD_BOUND = 195\n",
    "\n",
    "# discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# size of the replay buffer\n",
    "REPLAY_SIZE = 10000\n",
    "\n",
    "# warmup frames for the replay buffer\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# frequency for transferring weights from the actor DQN to the target DQN\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "\n",
    "# epsilon\n",
    "EPSILON_DECAY_LAST_FRAME = 15000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.01\n",
    "\n",
    "def train(net, target_net, env, buffer, agent, device, writer):\n",
    "\n",
    "    # epsilon starts from the initial value and is then annealed\n",
    "    epsilon = EPSILON_START\n",
    "\n",
    "    # instantiate the optimizer. Note that target_net is not optimized\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    frame_idx = 0\n",
    "\n",
    "    # frame idx and time at which the last episode ended\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_m_reward = None\n",
    "\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "\n",
    "        # compute the current epsilon with linear annealing\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "        # perform a step in the environment to gather experience\n",
    "        # reward, rbg_image = agent.play_step(net, epsilon, device=device)\n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "\n",
    "        # if the current episode has ended\n",
    "        if reward is not None:\n",
    "\n",
    "            # register the current total reward\n",
    "            total_rewards.append(reward)\n",
    "\n",
    "            # compute training speed\n",
    "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = frame_idx\n",
    "            ts = time.time()\n",
    "\n",
    "            # compute the mean reward over the last 100 episodes\n",
    "            # print(\"tot_ew\",total_rewards[-100:])\n",
    "            m_reward = np.mean(total_rewards[-100:])\n",
    "            # print(\"%d: done %d games, reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "            #     frame_idx, len(total_rewards), m_reward, epsilon, speed\n",
    "            # ))\n",
    "\n",
    "            # log values\n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "\n",
    "            # update best rewards\n",
    "            if best_m_reward is None or best_m_reward < m_reward:\n",
    "                #torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
    "                if best_m_reward is not None:\n",
    "                    print(\"Best reward updated %.3f -> %.3f\" % (best_m_reward, m_reward))\n",
    "                best_m_reward = m_reward\n",
    "\n",
    "            # stop training when a certain reward is achieved\n",
    "            if m_reward > MEAN_REWARD_BOUND:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "\n",
    "        # continue to collect experience until the warmup finishes\n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "\n",
    "        # at regular intervals load the weights of the actor DQN into the target DQN\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        # perform an optimization step\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        loss_t = calc_loss(batch, net, target_net, device=device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GHidO2pRaAHK"
   },
   "outputs": [],
   "source": [
    "def save_episode(net, env, agent, device, writer):\n",
    "\n",
    "    from IPython import display as ipythondisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()\n",
    "\n",
    "    # reset the environment\n",
    "    agent._reset()\n",
    "    all_frames = []\n",
    "\n",
    "    reward = None\n",
    "\n",
    "    # play an episode\n",
    "    while reward is None:\n",
    "\n",
    "        reward, rgb_image = agent.play_step(net, epsilon=0.0, device=device)\n",
    "\n",
    "        # change fromat from (H, W, C) to (C, H, W) and saves the image\n",
    "        rgb_image = torch.from_numpy(np.copy(rgb_image)).permute(2, 0, 1)\n",
    "        all_frames.append(rgb_image)\n",
    "\n",
    "    # video must be put into (batch, time, C, H, W) format to be saved\n",
    "    video = torch.stack(all_frames, dim=0).unsqueeze(0)\n",
    "    # save the video\n",
    "    writer.add_video(\"sample_episode\", video, global_step=0, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9shkETtGaFl8"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # build the environment\n",
    "    env = gym.make(DEFAULT_ENV_NAME, split=\"val\")\n",
    "\n",
    "    # create actor and target DQN models\n",
    "    net = DQN(env.observation_space['agent'].shape[0], env.action_space.n).to(device)\n",
    "    target_net = DQN(env.observation_space['agent'].shape[0], env.action_space.n).to(device)\n",
    "\n",
    "    # net = DQN(len(env.observation_space), env.action_space.n).to(device)\n",
    "    # target_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "\n",
    "    # initialize the logger\n",
    "    writer = SummaryWriter(log_dir=\"runs\")\n",
    "\n",
    "    # instantiate the experience buffer and the agent that collects experience\n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer)\n",
    "\n",
    "    # train the network\n",
    "    train(net, target_net, env, buffer, agent, device, writer)\n",
    "\n",
    "    # save a sample episode\n",
    "    # save_episode(net, env, agent, device, writer)\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MCTbsP1PaIA8"
   },
   "outputs": [],
   "source": [
    "!rm -r runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5FZ2VopqaJzn"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, target_net, device=\"cpu\"):\n",
    "\n",
    "    # unpack the batch\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "    \n",
    "    \n",
    "    # WARNING: fix non verificato\n",
    "    l = []\n",
    "    for dict in states:\n",
    "        l.append(dict[\"agent\"])\n",
    "    \n",
    "    # states_v = torch.tensor(np.array(states, copy=False)).to(device)\n",
    "    states_v = torch.tensor(np.array(l)).to(device)\n",
    "    next_states_v = torch.tensor(np.array(next_states, copy=False)).to(device)\n",
    "    # print(\"states\" ,states_v)\n",
    "    # print(\"next\" ,next_states_v)\n",
    "    # transform the batch elements to tensors\n",
    "\n",
    "    \n",
    "    \n",
    "    # states_v = torch.from_numpy(states).to(device)\n",
    "    # states_v = states[:][\"agent\"]\n",
    "    # next_states_v = torch.from_numpy(next_states).to(device)\n",
    "    \n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    # infer the qvalues for the states\n",
    "    state_qvalues = net(states_v)\n",
    "\n",
    "    \n",
    "    print(\"size before \",state_qvalues.shape)\n",
    "    # extract the qvalues for the action that was selected\n",
    "    #WARNING : SQUEEZE OF state_q_values and gather dim =0 \n",
    "    state_qvalues = state_qvalues.squeeze()\n",
    "\n",
    "    state_action_qvalues = state_qvalues.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    print(\"state qvalue \\n\", state_qvalues)\n",
    "    print(\"unsqueezed actions \\n\", actions_v.unsqueeze(-1))\n",
    "    print(\"result: \",state_action_qvalues)\n",
    "    with torch.no_grad():\n",
    " \n",
    "        # compute the qvalues for the next states using the target DQN\n",
    "        next_state_qvalues = target_net(next_states_v)\n",
    "\n",
    "        # extract the maximum one\n",
    "        next_state_max_qvalue = next_state_qvalues.max(dim=1)[0]\n",
    "\n",
    "        # if the next state refers to an ended episode, it has no value\n",
    "        next_state_max_qvalue[done_mask] = 0.0\n",
    "\n",
    "        next_state_max_qvalue = next_state_max_qvalue.detach()\n",
    "\n",
    "    # Computes the expected qvalue using the Bellman equation\n",
    "    # print(rewards_v.shape)\n",
    "    # print(next_state_max_qvalue.shape)\n",
    "    expected_state_action_qvalues = rewards_v + GAMMA * next_state_max_qvalue\n",
    "\n",
    "    # Penalizes the DQN for inferring a qvalue different from the one\n",
    "    # computed with the target DQN using the Bellman equation\n",
    "    return nn.MSELoss()(state_action_qvalues, expected_state_action_qvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gDDKIPXZaOjp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120796/2239051718.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_state['agent'] = torch.tensor(new_state['agent']).clone().detach().float()\n",
      "/tmp/ipykernel_120796/2239051718.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(self.state['agent']).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Best reward updated -47.871 -> -46.142\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024])\n",
      "Output shape: torch.Size([1, 1024])\n",
      "Best reward updated -46.142 -> -38.390\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024])\n",
      "Output shape: torch.Size([1, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Best reward updated -38.390 -> -38.345\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024])\n",
      "Output shape: torch.Size([1, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024])\n",
      "Output shape: torch.Size([1, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Best reward updated -38.345 -> -38.247\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024])\n",
      "Output shape: torch.Size([1, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024])\n",
      "Output shape: torch.Size([1, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024])\n",
      "Output shape: torch.Size([2, 1024])\n",
      "size before  torch.Size([32, 1, 9])\n",
      "state qvalue \n",
      " tensor([[-10.9493,  -9.2530,   4.1152,  -5.6684,   1.2995,   5.8684,  -2.8789,\n",
      "          -5.0366,   4.2051],\n",
      "        [ -3.8038,  -3.2510,   1.3098,  -1.9077,   0.7253,   2.1539,  -0.5068,\n",
      "          -1.8906,   1.2523],\n",
      "        [ -1.2737,  -0.6634,  -1.2151,  -0.3875,   1.3253,   1.1411,  -0.7767,\n",
      "          -1.6068,   0.8521],\n",
      "        [ -2.7641,  -1.8661,   0.3078,  -0.8412,   0.5466,   1.6160,  -0.2179,\n",
      "          -1.5196,   0.6820],\n",
      "        [-13.9141, -11.7697,   4.9741,  -6.8358,   1.8996,   7.4266,  -3.4020,\n",
      "          -6.5866,   5.3723],\n",
      "        [ -4.9208,  -3.9021,   1.4399,  -2.4542,   1.0218,   2.9288,  -0.7887,\n",
      "          -2.5819,   1.8064],\n",
      "        [ -3.1338,  -2.2125,  -0.0548,  -1.2038,   0.8042,   1.5521,  -0.1884,\n",
      "          -2.1319,   1.2380],\n",
      "        [ -0.6412,  -0.3759,  -1.5909,  -0.3330,   1.0195,  -0.1681,  -0.6985,\n",
      "          -1.1713,   1.3023],\n",
      "        [ -3.9179,  -2.9385,   0.6644,  -1.8378,   0.9669,   2.4692,  -0.5602,\n",
      "          -2.2482,   1.3885],\n",
      "        [ -9.4834,  -7.7173,   2.8878,  -4.6187,   1.5220,   5.4051,  -2.1098,\n",
      "          -4.6082,   3.5182],\n",
      "        [ -5.8209,  -4.5837,   1.8698,  -2.9621,   1.0892,   3.5728,  -1.0485,\n",
      "          -2.8409,   2.0981],\n",
      "        [ -4.5132,  -3.8966,   1.9934,  -2.1171,   0.8872,   2.4225,  -0.1581,\n",
      "          -2.3841,   1.4710],\n",
      "        [-10.9493,  -9.2530,   4.1152,  -5.6684,   1.2995,   5.8684,  -2.8789,\n",
      "          -5.0366,   4.2051],\n",
      "        [ -1.7464,  -1.3003,  -0.7440,  -0.0900,   1.5302,   1.1981,  -0.2414,\n",
      "          -1.9395,   0.7346],\n",
      "        [ -0.8112,  -0.4368,  -1.2003,  -0.2148,   1.1060,   0.5186,  -0.6173,\n",
      "          -1.2846,   0.9324],\n",
      "        [ -4.4976,  -3.1667,   0.8545,  -2.0074,   0.8848,   2.7912,  -0.7008,\n",
      "          -2.4153,   1.5630],\n",
      "        [ -3.9179,  -2.9385,   0.6644,  -1.8378,   0.9669,   2.4692,  -0.5602,\n",
      "          -2.2482,   1.3885],\n",
      "        [ -1.2254,  -0.5463,  -1.2725,  -0.3846,   1.2154,   0.3268,  -0.3028,\n",
      "          -1.5427,   1.3323],\n",
      "        [ -0.8112,  -0.4368,  -1.2003,  -0.2148,   1.1060,   0.5186,  -0.6173,\n",
      "          -1.2846,   0.9324],\n",
      "        [-10.9493,  -9.2530,   4.1152,  -5.6684,   1.2995,   5.8684,  -2.8789,\n",
      "          -5.0366,   4.2051],\n",
      "        [ -1.6167,  -1.0461,  -0.7409,  -0.2402,   1.2838,   0.8971,  -0.1286,\n",
      "          -1.7443,   0.8997],\n",
      "        [ -3.9179,  -2.9385,   0.6644,  -1.8378,   0.9669,   2.4692,  -0.5602,\n",
      "          -2.2482,   1.3885],\n",
      "        [ -5.5070,  -4.4549,   1.8081,  -2.7695,   0.9592,   3.2635,  -1.0328,\n",
      "          -2.6700,   1.9917],\n",
      "        [ -0.8286,  -0.4175,  -1.2590,  -0.2494,   1.1425,   0.6657,  -0.7298,\n",
      "          -1.3296,   0.9547],\n",
      "        [ -0.5381,  -0.6032,  -0.1727,  -0.2258,   0.4478,  -0.6280,  -0.4416,\n",
      "          -1.1360,   0.2974],\n",
      "        [ -5.8209,  -4.5837,   1.8698,  -2.9621,   1.0892,   3.5728,  -1.0485,\n",
      "          -2.8409,   2.0981],\n",
      "        [ -4.8758,  -3.6417,   1.4545,  -2.6034,   0.7327,   3.1026,  -1.0136,\n",
      "          -2.3013,   1.8299],\n",
      "        [ -4.4445,  -2.9495,   0.8451,  -1.7814,   0.7376,   2.6669,  -0.5731,\n",
      "          -2.3234,   1.4425],\n",
      "        [ -5.2169,  -4.3455,   1.9832,  -2.8094,   0.5780,   2.8173,  -1.4568,\n",
      "          -2.4442,   1.9822],\n",
      "        [ -0.6412,  -0.3759,  -1.5909,  -0.3330,   1.0195,  -0.1681,  -0.6985,\n",
      "          -1.1713,   1.3023],\n",
      "        [ -6.7185,  -5.4518,   2.2701,  -3.3490,   1.0228,   3.9404,  -1.3576,\n",
      "          -3.1149,   2.3921],\n",
      "        [ -4.9208,  -3.9021,   1.4399,  -2.4542,   1.0218,   2.9288,  -0.7887,\n",
      "          -2.5819,   1.8064]], grad_fn=<SqueezeBackward0>)\n",
      "unsqueezed actions \n",
      " tensor([[5],\n",
      "        [2],\n",
      "        [8],\n",
      "        [8],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [4],\n",
      "        [7],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [5],\n",
      "        [8],\n",
      "        [0],\n",
      "        [7],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [8],\n",
      "        [2],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "result:  tensor([  5.8684,   1.3098,   0.8521,   0.6820,  -3.4020,  -4.9208,   1.5521,\n",
      "         -0.3759,   2.4692,   5.4051,   1.0892,  -2.3841,  -2.8789,   1.5302,\n",
      "         -0.4368,   2.7912,   2.4692,  -1.2254,  -0.2148, -10.9493,   0.8971,\n",
      "          1.3885,  -5.5070,  -1.3296,  -0.6280,  -4.5837,   0.7327,   1.4425,\n",
      "          1.9832,  -1.5909,  -3.3490,   1.0218], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (9) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(env, buffer)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# train the network\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# save a sample episode\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# save_episode(net, env, agent, device, writer)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[11], line 101\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, target_net, env, buffer, agent, device, writer)\u001b[0m\n\u001b[1;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    100\u001b[0m batch \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39msample(BATCH_SIZE)\n\u001b[0;32m--> 101\u001b[0m loss_t \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m loss_t\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[18], line 58\u001b[0m, in \u001b[0;36mcalc_loss\u001b[0;34m(batch, net, target_net, device)\u001b[0m\n\u001b[1;32m     53\u001b[0m     next_state_max_qvalue \u001b[38;5;241m=\u001b[39m next_state_max_qvalue\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Computes the expected qvalue using the Bellman equation\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(rewards_v.shape)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# print(next_state_max_qvalue.shape)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m expected_state_action_qvalues \u001b[38;5;241m=\u001b[39m \u001b[43mrewards_v\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnext_state_max_qvalue\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Penalizes the DQN for inferring a qvalue different from the one\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# computed with the target DQN using the Bellman equation\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mMSELoss()(state_action_qvalues, expected_state_action_qvalues)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (9) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK REWARD VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CU0-J0Tt-qY"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# plt.plot(logs[\"reward\"])\n",
    "# plt.title(\"training rewards (average)\")\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.plot(logs[\"step_count\"])\n",
    "# plt.title(\"Max step count (training)\")\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.plot(logs[\"eval reward (sum)\"])\n",
    "# plt.title(\"Return (test)\")\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.plot(logs[\"eval step_count\"])\n",
    "# plt.title(\"Max step count (test)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([{\"a\":2},{\"b\":23}])\n",
    "# for i in range(x.size[0]):\n",
    "#     print(x[i][\"a\"])\n",
    "# # \n",
    "\n",
    "from torch import tensor\n",
    "\n",
    "states= np.array([{'agent': tensor([[896.,   0., 639., 271.]]), 'target': tensor([[305.9500,  49.4000, 435.6600, 203.5200]])},\n",
    " {'agent': tensor([[377.,   0., 477., 576.]]), 'target': tensor([[  0.0000,   0.0000, 128.0000, 152.4500]])},\n",
    " {'agent': tensor([[426.,   0., 477., 512.]]), 'target': tensor([[259.1300, 120.0000, 336.0200, 363.2300]])},\n",
    " {'agent': tensor([[850., 150., 499., 374.]]), 'target': tensor([[270.5100,  19.3800, 410.3900, 168.5400]])},\n",
    " {'agent': tensor([[253.,   0., 424., 639.]]), 'target': tensor([[ 86.2600, 112.8500, 259.1900, 235.8500]])},\n",
    " {'agent': tensor([[426.,   0., 477., 512.]]), 'target': tensor([[259.1300, 120.0000, 336.0200, 363.2300]])},\n",
    " {'agent': tensor([[ 64.,   0., 639., 432.]]), 'target': tensor([[146.7000, 102.5400, 501.5700, 280.5200]])},\n",
    " {'agent': tensor([[832.,   0., 639., 479.]]), 'target': tensor([[378.0300,   4.7400, 640.0000, 256.1700]])},\n",
    " {'agent': tensor([[253.,   0., 424., 639.]]), 'target': tensor([[ 86.2600, 112.8500, 259.1900, 235.8500]])},\n",
    " {'agent': tensor([[ 88.,   0., 443., 639.]]), 'target': tensor([[ 43.2200,  78.1300, 204.4700, 623.3800]])} ])\n",
    "\n",
    "\n",
    "y = torch.empty\n",
    "l = []\n",
    "for dict in states:\n",
    "    l.append(dict[\"agent\"])\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0])\n",
      "tensor([[1],\n",
      "        [0]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(t)\n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m tensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m      7\u001b[0m              [\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m]])\n\u001b[0;32m----> 8\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "from torch import tensor\n",
    "t = tensor([1,0])\n",
    "print(t)\n",
    "t = t.unsqueeze(-1)\n",
    "print(t)\n",
    "x = tensor([[1,2,3],\n",
    "             [4,5,6]])\n",
    "Y = x[0].gather(1, t)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeprl-MQ4pjQ9z-py3.11",
   "language": "python",
   "name": "deeprl-mq4pjq9z-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
